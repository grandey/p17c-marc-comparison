{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis_cdo_nco_draft2017b.ipynb\n",
    "\n",
    "## Purpose\n",
    "Use CDO and NCO to analyse CESM simulation output from project [p17c-marc-comparison](https://github.com/grandey/p17c-marc-comparison).\n",
    "\n",
    "## Requirements\n",
    "- Climate Data Operators (CDO)\n",
    "- NetCDF Operators (NCO)\n",
    "- CESM output data, post-processed to time-series format, as described in [data_management.org](https://github.com/grandey/p17c-marc-comparison/blob/master/manage_data/data_management.org#syncing-to-local-machine-for-analysis). These data are available via https://doi.org/10.6084/m9.figshare.5687812.\n",
    "\n",
    "## Author\n",
    "Benjamin S. Grandey, 2017-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 12 16:31:43 +08 2018\r\n"
     ]
    }
   ],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDO and NCO version information\n",
    "Useful to record for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Data Operators version 1.9.1 (http://mpimet.mpg.de/cdo)\n",
      "Compiled: by root on squall2.local (x86_64-apple-darwin17.2.0) Nov  2 2017 18:28:19\n",
      "CXX Compiler: /usr/bin/clang++ -std=gnu++11 -pipe -Os -stdlib=libc++ -arch x86_64  -D_THREAD_SAFE -pthread\n",
      "CXX version : unknown\n",
      "C Compiler: /usr/bin/clang -pipe -Os -arch x86_64  -D_THREAD_SAFE -pthread\n",
      "C version : unknown\n",
      "Features: DATA PTHREADS HDF5 NC4/HDF5 OPeNDAP SZ UDUNITS2 PROJ.4 CURL FFTW3 SSE4_1\n",
      "Libraries: HDF5/1.10.1 proj/4.93 curl/7.56.1\n",
      "Filetypes: srv ext ieg grb1 nc1 nc2 nc4 nc4c nc5 \n",
      "     CDI library version : 1.9.1 of Nov  2 2017 18:27:49\n",
      " CGRIBEX library version : 1.9.0 of Sep 29 2017 10:16:02\n",
      "  NetCDF library version : 4.4.1.1 of Oct  6 2017 14:14:42 $\n",
      "    HDF5 library version : 1.10.1\n",
      " SERVICE library version : 1.4.0 of Nov  2 2017 18:27:47\n",
      "   EXTRA library version : 1.4.0 of Nov  2 2017 18:27:46\n",
      "     IEG library version : 1.4.0 of Nov  2 2017 18:27:46\n",
      "    FILE library version : 1.8.3 of Nov  2 2017 18:27:46\n",
      "\n",
      "NCO netCDF Operators version 4.6.6 built by root on squall2.local at Nov  3 2017 12:13:40\n",
      "ncks version 4.6.6\n"
     ]
    }
   ],
   "source": [
    "! cdo --version\n",
    "! ncks --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory locations for input and output NetCDF files\n",
    "The data in the input directory (*in_dir*) are available via Figshare: https://doi.org/10.6084/m9.figshare.5687812."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data directory\n",
    "#in_dir = os.path.expandvars('$HOME/data/figshare/figshare5687812/')\n",
    "in_dir = os.path.expandvars('$HOME/data/projects/p17c_marc_comparison/output_timeseries/')\n",
    "\n",
    "# Output data directory\n",
    "out_dir = os.path.expandvars('$HOME/data/projects/p17c_marc_comparison/analysis_cdo_nco_draft2017b/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean output data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 12 16:31:43 +08 2018\r\n"
     ]
    }
   ],
   "source": [
    "for filename in glob('{}/*.nc'.format(out_dir)):\n",
    "    print('Deleting {}'.format(filename.split('/')[-1]))\n",
    "    os.remove(filename)\n",
    "for filename in glob('{}/*.nco'.format(out_dir)):\n",
    "    print('Deleting {}'.format(filename.split('/')[-1]))\n",
    "    os.remove(filename)\n",
    "for filename in glob('{}/*.tmp'.format(out_dir)):\n",
    "    print('Deleting {}'.format(filename.split('/')[-1]))\n",
    "    os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate annual means of standard 2D atmosphere variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OC_LDG, marc, 1850\n",
      "  Written marc_1850_OC_LDG_ANN.nc\n",
      "OC_LDG, marc, 2000\n",
      "  Written marc_2000_OC_LDG_ANN.nc\n",
      "BURDENSO4, mam3, 1850\n",
      "  Written mam3_1850_BURDENSO4_ANN.nc\n",
      "BURDENSO4, mam3, 2000\n",
      "  Written mam3_2000_BURDENSO4_ANN.nc\n",
      "BURDENSO4, mam7, 1850\n",
      "  Written mam7_1850_BURDENSO4_ANN.nc\n",
      "BURDENSO4, mam7, 2000\n",
      "  Written mam7_2000_BURDENSO4_ANN.nc\n",
      "BURDENPOM, mam3, 1850\n",
      "  Written mam3_1850_BURDENPOM_ANN.nc\n",
      "BURDENPOM, mam3, 2000\n",
      "  Written mam3_2000_BURDENPOM_ANN.nc\n",
      "BURDENPOM, mam7, 1850\n",
      "  Written mam7_1850_BURDENPOM_ANN.nc\n",
      "BURDENPOM, mam7, 2000\n",
      "  Written mam7_2000_BURDENPOM_ANN.nc\n",
      "BURDENBC, mam3, 1850\n",
      "  Written mam3_1850_BURDENBC_ANN.nc\n",
      "BURDENBC, mam3, 2000\n",
      "  Written mam3_2000_BURDENBC_ANN.nc\n",
      "BURDENBC, mam7, 1850\n",
      "  Written mam7_1850_BURDENBC_ANN.nc\n",
      "BURDENBC, mam7, 2000\n",
      "  Written mam7_2000_BURDENBC_ANN.nc\n",
      "BURDENSEASALT, mam3, 1850\n",
      "  Written mam3_1850_BURDENSEASALT_ANN.nc\n",
      "BURDENSEASALT, mam3, 2000\n",
      "  Written mam3_2000_BURDENSEASALT_ANN.nc\n",
      "BURDENSEASALT, mam7, 1850\n",
      "  Written mam7_1850_BURDENSEASALT_ANN.nc\n",
      "BURDENSEASALT, mam7, 2000\n",
      "  Written mam7_2000_BURDENSEASALT_ANN.nc\n",
      "BURDENDUST, mam3, 1850\n",
      "  Written mam3_1850_BURDENDUST_ANN.nc\n",
      "BURDENDUST, mam3, 2000\n",
      "  Written mam3_2000_BURDENDUST_ANN.nc\n",
      "BURDENDUST, mam7, 1850\n",
      "  Written mam7_1850_BURDENDUST_ANN.nc\n",
      "BURDENDUST, mam7, 2000\n",
      "  Written mam7_2000_BURDENDUST_ANN.nc\n",
      "TAU_tot, marc, 1850\n",
      "  Written marc_1850_TAU_tot_ANN.nc\n",
      "TAU_tot, marc, 2000\n",
      "  Written marc_2000_TAU_tot_ANN.nc\n",
      "AEROD_v, mam3, 1850\n",
      "  Written mam3_1850_AEROD_v_ANN.nc\n",
      "AEROD_v, mam3, 2000\n",
      "  Written mam3_2000_AEROD_v_ANN.nc\n",
      "AEROD_v, mam7, 1850\n",
      "  Written mam7_1850_AEROD_v_ANN.nc\n",
      "AEROD_v, mam7, 2000\n",
      "  Written mam7_2000_AEROD_v_ANN.nc\n",
      "CDNUMC, marc, 1850\n",
      "  Written marc_1850_CDNUMC_ANN.nc\n",
      "CDNUMC, marc, 2000\n",
      "  Written marc_2000_CDNUMC_ANN.nc\n",
      "CDNUMC, mam3, 1850\n",
      "  Written mam3_1850_CDNUMC_ANN.nc\n",
      "CDNUMC, mam3, 2000\n",
      "  Written mam3_2000_CDNUMC_ANN.nc\n",
      "CDNUMC, mam7, 1850\n",
      "  Written mam7_1850_CDNUMC_ANN.nc\n",
      "CDNUMC, mam7, 2000\n",
      "  Written mam7_2000_CDNUMC_ANN.nc\n",
      "CLDTOT, marc, 1850\n",
      "  Written marc_1850_CLDTOT_ANN.nc\n",
      "CLDTOT, marc, 2000\n",
      "  Written marc_2000_CLDTOT_ANN.nc\n",
      "CLDTOT, mam3, 1850\n",
      "  Written mam3_1850_CLDTOT_ANN.nc\n",
      "CLDTOT, mam3, 2000\n",
      "  Written mam3_2000_CLDTOT_ANN.nc\n",
      "CLDTOT, mam7, 1850\n",
      "  Written mam7_1850_CLDTOT_ANN.nc\n",
      "CLDTOT, mam7, 2000\n",
      "  Written mam7_2000_CLDTOT_ANN.nc\n",
      "CLDLOW, marc, 1850\n",
      "  Written marc_1850_CLDLOW_ANN.nc\n",
      "CLDLOW, marc, 2000\n",
      "  Written marc_2000_CLDLOW_ANN.nc\n",
      "CLDLOW, mam3, 1850\n",
      "  Written mam3_1850_CLDLOW_ANN.nc\n",
      "CLDLOW, mam3, 2000\n",
      "  Written mam3_2000_CLDLOW_ANN.nc\n",
      "CLDLOW, mam7, 1850\n",
      "  Written mam7_1850_CLDLOW_ANN.nc\n",
      "CLDLOW, mam7, 2000\n",
      "  Written mam7_2000_CLDLOW_ANN.nc\n",
      "CLDMED, marc, 1850\n",
      "  Written marc_1850_CLDMED_ANN.nc\n",
      "CLDMED, marc, 2000\n",
      "  Written marc_2000_CLDMED_ANN.nc\n",
      "CLDMED, mam3, 1850\n",
      "  Written mam3_1850_CLDMED_ANN.nc\n",
      "CLDMED, mam3, 2000\n",
      "  Written mam3_2000_CLDMED_ANN.nc\n",
      "CLDMED, mam7, 1850\n",
      "  Written mam7_1850_CLDMED_ANN.nc\n",
      "CLDMED, mam7, 2000\n",
      "  Written mam7_2000_CLDMED_ANN.nc\n",
      "CLDHGH, marc, 1850\n",
      "  Written marc_1850_CLDHGH_ANN.nc\n",
      "CLDHGH, marc, 2000\n",
      "  Written marc_2000_CLDHGH_ANN.nc\n",
      "CLDHGH, mam3, 1850\n",
      "  Written mam3_1850_CLDHGH_ANN.nc\n",
      "CLDHGH, mam3, 2000\n",
      "  Written mam3_2000_CLDHGH_ANN.nc\n",
      "CLDHGH, mam7, 1850\n",
      "  Written mam7_1850_CLDHGH_ANN.nc\n",
      "CLDHGH, mam7, 2000\n",
      "  Written mam7_2000_CLDHGH_ANN.nc\n",
      "TGCLDLWP, marc, 1850\n",
      "  Written marc_1850_TGCLDLWP_ANN.nc\n",
      "TGCLDLWP, marc, 2000\n",
      "  Written marc_2000_TGCLDLWP_ANN.nc\n",
      "TGCLDLWP, mam3, 1850\n",
      "  Written mam3_1850_TGCLDLWP_ANN.nc\n",
      "TGCLDLWP, mam3, 2000\n",
      "  Written mam3_2000_TGCLDLWP_ANN.nc\n",
      "TGCLDLWP, mam7, 1850\n",
      "  Written mam7_1850_TGCLDLWP_ANN.nc\n",
      "TGCLDLWP, mam7, 2000\n",
      "  Written mam7_2000_TGCLDLWP_ANN.nc\n",
      "TGCLDIWP, marc, 1850\n",
      "  Written marc_1850_TGCLDIWP_ANN.nc\n",
      "TGCLDIWP, marc, 2000\n",
      "  Written marc_2000_TGCLDIWP_ANN.nc\n",
      "TGCLDIWP, mam3, 1850\n",
      "  Written mam3_1850_TGCLDIWP_ANN.nc\n",
      "TGCLDIWP, mam3, 2000\n",
      "  Written mam3_2000_TGCLDIWP_ANN.nc\n",
      "TGCLDIWP, mam7, 1850\n",
      "  Written mam7_1850_TGCLDIWP_ANN.nc\n",
      "TGCLDIWP, mam7, 2000\n",
      "  Written mam7_2000_TGCLDIWP_ANN.nc\n",
      "TGCLDCWP, marc, 1850\n",
      "  Written marc_1850_TGCLDCWP_ANN.nc\n",
      "TGCLDCWP, marc, 2000\n",
      "  Written marc_2000_TGCLDCWP_ANN.nc\n",
      "TGCLDCWP, mam3, 1850\n",
      "  Written mam3_1850_TGCLDCWP_ANN.nc\n",
      "TGCLDCWP, mam3, 2000\n",
      "  Written mam3_2000_TGCLDCWP_ANN.nc\n",
      "TGCLDCWP, mam7, 1850\n",
      "  Written mam7_1850_TGCLDCWP_ANN.nc\n",
      "TGCLDCWP, mam7, 2000\n",
      "  Written mam7_2000_TGCLDCWP_ANN.nc\n",
      "PRECC, marc, 1850\n",
      "  Written marc_1850_PRECC_ANN.nc\n",
      "PRECC, marc, 2000\n",
      "  Written marc_2000_PRECC_ANN.nc\n",
      "PRECC, mam3, 1850\n",
      "  Written mam3_1850_PRECC_ANN.nc\n",
      "PRECC, mam3, 2000\n",
      "  Written mam3_2000_PRECC_ANN.nc\n",
      "PRECC, mam7, 1850\n",
      "  Written mam7_1850_PRECC_ANN.nc\n",
      "PRECC, mam7, 2000\n",
      "  Written mam7_2000_PRECC_ANN.nc\n",
      "PRECL, marc, 1850\n",
      "  Written marc_1850_PRECL_ANN.nc\n",
      "PRECL, marc, 2000\n",
      "  Written marc_2000_PRECL_ANN.nc\n",
      "PRECL, mam3, 1850\n",
      "  Written mam3_1850_PRECL_ANN.nc\n",
      "PRECL, mam3, 2000\n",
      "  Written mam3_2000_PRECL_ANN.nc\n",
      "PRECL, mam7, 1850\n",
      "  Written mam7_1850_PRECL_ANN.nc\n",
      "PRECL, mam7, 2000\n",
      "  Written mam7_2000_PRECL_ANN.nc\n",
      "PRECSC, marc, 1850\n",
      "  Written marc_1850_PRECSC_ANN.nc\n",
      "PRECSC, marc, 2000\n",
      "  Written marc_2000_PRECSC_ANN.nc\n",
      "PRECSC, mam3, 1850\n",
      "  Written mam3_1850_PRECSC_ANN.nc\n",
      "PRECSC, mam3, 2000\n",
      "  Written mam3_2000_PRECSC_ANN.nc\n",
      "PRECSC, mam7, 1850\n",
      "  Written mam7_1850_PRECSC_ANN.nc\n",
      "PRECSC, mam7, 2000\n",
      "  Written mam7_2000_PRECSC_ANN.nc\n",
      "PRECSL, marc, 1850\n",
      "  Written marc_1850_PRECSL_ANN.nc\n",
      "PRECSL, marc, 2000\n",
      "  Written marc_2000_PRECSL_ANN.nc\n",
      "PRECSL, mam3, 1850\n",
      "  Written mam3_1850_PRECSL_ANN.nc\n",
      "PRECSL, mam3, 2000\n",
      "  Written mam3_2000_PRECSL_ANN.nc\n",
      "PRECSL, mam7, 1850\n",
      "  Written mam7_1850_PRECSL_ANN.nc\n",
      "PRECSL, mam7, 2000\n",
      "  Written mam7_2000_PRECSL_ANN.nc\n",
      "U10, marc, 1850\n",
      "  Written marc_1850_U10_ANN.nc\n",
      "U10, marc, 2000\n",
      "  Written marc_2000_U10_ANN.nc\n",
      "U10, mam3, 1850\n",
      "  Written mam3_1850_U10_ANN.nc\n",
      "U10, mam3, 2000\n",
      "  Written mam3_2000_U10_ANN.nc\n",
      "U10, mam7, 1850\n",
      "  Written mam7_1850_U10_ANN.nc\n",
      "U10, mam7, 2000\n",
      "  Written mam7_2000_U10_ANN.nc\n",
      "FSNTOA, marc, 1850\n",
      "  Written marc_1850_FSNTOA_ANN.nc\n",
      "FSNTOA, marc, 2000\n",
      "  Written marc_2000_FSNTOA_ANN.nc\n",
      "FSNTOA, mam3, 1850\n",
      "  Written mam3_1850_FSNTOA_ANN.nc\n",
      "FSNTOA, mam3, 2000\n",
      "  Written mam3_2000_FSNTOA_ANN.nc\n",
      "FSNTOA, mam7, 1850\n",
      "  Written mam7_1850_FSNTOA_ANN.nc\n",
      "FSNTOA, mam7, 2000\n",
      "  Written mam7_2000_FSNTOA_ANN.nc\n",
      "FSNTOANOA, marc, 1850\n",
      "  Written marc_1850_FSNTOANOA_ANN.nc\n",
      "FSNTOANOA, marc, 2000\n",
      "  Written marc_2000_FSNTOANOA_ANN.nc\n",
      "FSNTOA_d1, mam3, 1850\n",
      "  Written mam3_1850_FSNTOA_d1_ANN.nc\n",
      "FSNTOA_d1, mam3, 2000\n",
      "  Written mam3_2000_FSNTOA_d1_ANN.nc\n",
      "FSNTOA_d1, mam7, 1850\n",
      "  Written mam7_1850_FSNTOA_d1_ANN.nc\n",
      "FSNTOA_d1, mam7, 2000\n",
      "  Written mam7_2000_FSNTOA_d1_ANN.nc\n",
      "FSNS, marc, 1850\n",
      "  Written marc_1850_FSNS_ANN.nc\n",
      "FSNS, marc, 2000\n",
      "  Written marc_2000_FSNS_ANN.nc\n",
      "FSNS, mam3, 1850\n",
      "  Written mam3_1850_FSNS_ANN.nc\n",
      "FSNS, mam3, 2000\n",
      "  Written mam3_2000_FSNS_ANN.nc\n",
      "FSNS, mam7, 1850\n",
      "  Written mam7_1850_FSNS_ANN.nc\n",
      "FSNS, mam7, 2000\n",
      "  Written mam7_2000_FSNS_ANN.nc\n",
      "FSNSNOA, marc, 1850\n",
      "  Written marc_1850_FSNSNOA_ANN.nc\n",
      "FSNSNOA, marc, 2000\n",
      "  Written marc_2000_FSNSNOA_ANN.nc\n",
      "FSNS_d1, mam3, 1850\n",
      "  Written mam3_1850_FSNS_d1_ANN.nc\n",
      "FSNS_d1, mam3, 2000\n",
      "  Written mam3_2000_FSNS_d1_ANN.nc\n",
      "FSNS_d1, mam7, 1850\n",
      "  Written mam7_1850_FSNS_d1_ANN.nc\n",
      "FSNS_d1, mam7, 2000\n",
      "  Written mam7_2000_FSNS_d1_ANN.nc\n",
      "CRF, marc, 1850\n",
      "  Written marc_1850_CRF_ANN.nc\n",
      "CRF, marc, 2000\n",
      "  Written marc_2000_CRF_ANN.nc\n",
      "SWCF_d1, mam3, 1850\n",
      "  Written mam3_1850_SWCF_d1_ANN.nc\n",
      "SWCF_d1, mam3, 2000\n",
      "  Written mam3_2000_SWCF_d1_ANN.nc\n",
      "SWCF_d1, mam7, 1850\n",
      "  Written mam7_1850_SWCF_d1_ANN.nc\n",
      "SWCF_d1, mam7, 2000\n",
      "  Written mam7_2000_SWCF_d1_ANN.nc\n",
      "LWCF, marc, 1850\n",
      "  Written marc_1850_LWCF_ANN.nc\n",
      "LWCF, marc, 2000\n",
      "  Written marc_2000_LWCF_ANN.nc\n",
      "LWCF, mam3, 1850\n",
      "  Written mam3_1850_LWCF_ANN.nc\n",
      "LWCF, mam3, 2000\n",
      "  Written mam3_2000_LWCF_ANN.nc\n",
      "LWCF, mam7, 1850\n",
      "  Written mam7_1850_LWCF_ANN.nc\n",
      "LWCF, mam7, 2000\n",
      "  Written mam7_2000_LWCF_ANN.nc\n",
      "FSNTOACNOA, marc, 1850\n",
      "  Written marc_1850_FSNTOACNOA_ANN.nc\n",
      "FSNTOACNOA, marc, 2000\n",
      "  Written marc_2000_FSNTOACNOA_ANN.nc\n",
      "FSNTOAC_d1, mam3, 1850\n",
      "  Written mam3_1850_FSNTOAC_d1_ANN.nc\n",
      "FSNTOAC_d1, mam3, 2000\n",
      "  Written mam3_2000_FSNTOAC_d1_ANN.nc\n",
      "FSNTOAC_d1, mam7, 1850\n",
      "  Written mam7_1850_FSNTOAC_d1_ANN.nc\n",
      "FSNTOAC_d1, mam7, 2000\n",
      "  Written mam7_2000_FSNTOAC_d1_ANN.nc\n",
      "Tue Jun 12 16:33:32 +08 2018\n"
     ]
    }
   ],
   "source": [
    "variable_list = ['OC_LDG',  # MARC pure OC burden, for checking burdens derived using mass-mixing ratios\n",
    "                 'BURDENSO4', 'BURDENPOM', 'BURDENBC',  # MAM burdens\n",
    "                 'BURDENSEASALT', 'BURDENDUST',  # MAM burdens cont.\n",
    "                 'TAU_tot', 'AEROD_v',  # AOD in MARC, MAM\n",
    "                 'CDNUMC',  # vertically-integrated CDNC\n",
    "                 'CLDTOT', 'CLDLOW', 'CLDMED', 'CLDHGH',  # cloud fraction\n",
    "                 'TGCLDLWP', 'TGCLDIWP', 'TGCLDCWP',  # grid-box average water path\n",
    "                 'PRECC', 'PRECL',  # convective and large-scale precipitation rate (in m/s)\n",
    "                 'PRECSC', 'PRECSL',  # convective and large-scale snow rate (in m/s)\n",
    "                 'U10',  # 10m wind speed\n",
    "                 'FSNTOA', 'FSNTOANOA', 'FSNTOA_d1',  # SW flux at TOA, including clean-sky for MARC, MAM\n",
    "                 'FSNS', 'FSNSNOA', 'FSNS_d1',  # SW flux at surface, including clean-sky for MAM, MAM\n",
    "                 'CRF', 'SWCF_d1',  # SW cloud radiative effect in MARC, MAM\n",
    "                 'LWCF',  # LW cloud radiative effect \n",
    "                 'FSNTOACNOA', 'FSNTOAC_d1',  # SW clear-sky clean-sky flux at TOA in MARC, MAM\n",
    "                ]\n",
    "for variable in variable_list:\n",
    "    for model in ['marc', 'mam3', 'mam7']:\n",
    "        for year in ['1850', '2000']:\n",
    "            # Check if input file exists\n",
    "            in_filename = '{}/p17c_{}_{}.cam.h0.{}.nc'.format(in_dir, model, year, variable)\n",
    "            if os.path.isfile(in_filename):\n",
    "                print('{}, {}, {}'.format(variable, model, year))\n",
    "                # Calculate annual means using NCO (with years starting in January)\n",
    "                annual_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable)\n",
    "                ! ncra -O --mro -d time,,,12,12 {in_filename} {annual_filename}\n",
    "                print('  Written {}'.format(annual_filename.split('/')[-1]))\n",
    "! date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data on specific atmosphere model levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCN3, marc, 1850, ml30\n",
      "  Selecting data for model level 30\n",
      "  Renaming variable to CCN3_ml30\n",
      "  Calculating annual means\n",
      "  Written marc_1850_CCN3_ml30_ANN.nc\n",
      "  Removing temp_marc_1850_CCN3_ml30.nc\n",
      "CCN3, marc, 1850, ml24\n",
      "  Selecting data for model level 24\n",
      "  Renaming variable to CCN3_ml24\n",
      "  Calculating annual means\n",
      "  Written marc_1850_CCN3_ml24_ANN.nc\n",
      "  Removing temp_marc_1850_CCN3_ml24.nc\n",
      "CCN3, marc, 1850, ml19\n",
      "  Selecting data for model level 19\n",
      "  Renaming variable to CCN3_ml19\n",
      "  Calculating annual means\n",
      "  Written marc_1850_CCN3_ml19_ANN.nc\n",
      "  Removing temp_marc_1850_CCN3_ml19.nc\n",
      "CCN3, marc, 2000, ml30\n",
      "  Selecting data for model level 30\n",
      "  Renaming variable to CCN3_ml30\n",
      "  Calculating annual means\n",
      "  Written marc_2000_CCN3_ml30_ANN.nc\n",
      "  Removing temp_marc_2000_CCN3_ml30.nc\n",
      "CCN3, marc, 2000, ml24\n",
      "  Selecting data for model level 24\n",
      "  Renaming variable to CCN3_ml24\n",
      "  Calculating annual means\n",
      "  Written marc_2000_CCN3_ml24_ANN.nc\n",
      "  Removing temp_marc_2000_CCN3_ml24.nc\n",
      "CCN3, marc, 2000, ml19\n",
      "  Selecting data for model level 19\n",
      "  Renaming variable to CCN3_ml19\n",
      "  Calculating annual means\n",
      "  Written marc_2000_CCN3_ml19_ANN.nc\n",
      "  Removing temp_marc_2000_CCN3_ml19.nc\n",
      "CCN3, mam3, 1850, ml30\n",
      "  Selecting data for model level 30\n",
      "  Renaming variable to CCN3_ml30\n",
      "  Calculating annual means\n",
      "  Written mam3_1850_CCN3_ml30_ANN.nc\n",
      "  Removing temp_mam3_1850_CCN3_ml30.nc\n",
      "CCN3, mam3, 1850, ml24\n",
      "  Selecting data for model level 24\n",
      "  Renaming variable to CCN3_ml24\n",
      "  Calculating annual means\n",
      "  Written mam3_1850_CCN3_ml24_ANN.nc\n",
      "  Removing temp_mam3_1850_CCN3_ml24.nc\n",
      "CCN3, mam3, 1850, ml19\n",
      "  Selecting data for model level 19\n",
      "  Renaming variable to CCN3_ml19\n",
      "  Calculating annual means\n",
      "  Written mam3_1850_CCN3_ml19_ANN.nc\n",
      "  Removing temp_mam3_1850_CCN3_ml19.nc\n",
      "CCN3, mam3, 2000, ml30\n",
      "  Selecting data for model level 30\n",
      "  Renaming variable to CCN3_ml30\n",
      "  Calculating annual means\n",
      "  Written mam3_2000_CCN3_ml30_ANN.nc\n",
      "  Removing temp_mam3_2000_CCN3_ml30.nc\n",
      "CCN3, mam3, 2000, ml24\n",
      "  Selecting data for model level 24\n",
      "  Renaming variable to CCN3_ml24\n",
      "  Calculating annual means\n",
      "  Written mam3_2000_CCN3_ml24_ANN.nc\n",
      "  Removing temp_mam3_2000_CCN3_ml24.nc\n",
      "CCN3, mam3, 2000, ml19\n",
      "  Selecting data for model level 19\n",
      "  Renaming variable to CCN3_ml19\n",
      "  Calculating annual means\n",
      "  Written mam3_2000_CCN3_ml19_ANN.nc\n",
      "  Removing temp_mam3_2000_CCN3_ml19.nc\n",
      "CCN3, mam7, 1850, ml30\n",
      "  Selecting data for model level 30\n",
      "  Renaming variable to CCN3_ml30\n",
      "  Calculating annual means\n",
      "  Written mam7_1850_CCN3_ml30_ANN.nc\n",
      "  Removing temp_mam7_1850_CCN3_ml30.nc\n",
      "CCN3, mam7, 1850, ml24\n",
      "  Selecting data for model level 24\n",
      "  Renaming variable to CCN3_ml24\n",
      "  Calculating annual means\n",
      "  Written mam7_1850_CCN3_ml24_ANN.nc\n",
      "  Removing temp_mam7_1850_CCN3_ml24.nc\n",
      "CCN3, mam7, 1850, ml19\n",
      "  Selecting data for model level 19\n",
      "  Renaming variable to CCN3_ml19\n",
      "  Calculating annual means\n",
      "  Written mam7_1850_CCN3_ml19_ANN.nc\n",
      "  Removing temp_mam7_1850_CCN3_ml19.nc\n",
      "CCN3, mam7, 2000, ml30\n",
      "  Selecting data for model level 30\n",
      "  Renaming variable to CCN3_ml30\n",
      "  Calculating annual means\n",
      "  Written mam7_2000_CCN3_ml30_ANN.nc\n",
      "  Removing temp_mam7_2000_CCN3_ml30.nc\n",
      "CCN3, mam7, 2000, ml24\n",
      "  Selecting data for model level 24\n",
      "  Renaming variable to CCN3_ml24\n",
      "  Calculating annual means\n",
      "  Written mam7_2000_CCN3_ml24_ANN.nc\n",
      "  Removing temp_mam7_2000_CCN3_ml24.nc\n",
      "CCN3, mam7, 2000, ml19\n",
      "  Selecting data for model level 19\n",
      "  Renaming variable to CCN3_ml19\n",
      "  Calculating annual means\n",
      "  Written mam7_2000_CCN3_ml19_ANN.nc\n",
      "  Removing temp_mam7_2000_CCN3_ml19.nc\n",
      "Tue Jun 12 16:34:44 +08 2018\n"
     ]
    }
   ],
   "source": [
    "variable_list = ['CCN3', ]\n",
    "for variable in variable_list:\n",
    "    for model in ['marc', 'mam3', 'mam7']:\n",
    "        for year in ['1850', '2000']:\n",
    "            # Check if input file exists\n",
    "            in_filename = '{}/p17c_{}_{}.cam.h0.{}.nc'.format(in_dir, model, year, variable)\n",
    "            if os.path.isfile(in_filename):  # there is no mam7_1850 simulation\n",
    "                # Loop over model levels of interest\n",
    "                for level in [30, 24, 19]:  # 30 is bottom level; 24 is ~860hpa; 19 is ~525hPa\n",
    "                    print('{}, {}, {}, ml{}'.format(variable, model, year, level))\n",
    "                    # Select data for model level using CDO\n",
    "                    print('  Selecting data for model level {}'.format(level))\n",
    "                    level_filename = '{}/temp_{}_{}_{}_ml{}.nc'.format(out_dir, model, year, variable, level)\n",
    "                    ! cdo -s sellevidx,{level} {in_filename} {level_filename}\n",
    "                    # Rename variable using NCO\n",
    "                    print('  Renaming variable to {}_ml{}'.format(variable, level))\n",
    "                    ! ncrename -v {variable},{variable}_ml{level} {level_filename} >/dev/null 2>/dev/null\n",
    "                    # Calculate annual means using NCO (with years starting in January)\n",
    "                    print('  Calculating annual means')\n",
    "                    annual_filename = '{}/{}_{}_{}_ml{}_ANN.nc'.format(out_dir, model, year, variable, level)\n",
    "                    ! ncra -O --mro -d time,,,12,12 {level_filename} {annual_filename}\n",
    "                    print('  Written {}'.format(annual_filename.split('/')[-1]))\n",
    "                    # Remove temporary file\n",
    "                    for filename in [level_filename, ]:\n",
    "                        print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "                        os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate column loadings from MARC mass mixing ratios\n",
    "- This is to facilitate comparison with MAM. Although MARC diagnoses some column loadings, the column loading are not available for every aerosol component.\n",
    "- Regarding the calculation of the mass in each level, the following reference is helpful:  http://nco.sourceforge.net/nco.html#Left-hand-casting. A description of the hybrid vertical coordinate system can be found here: http://www.cesm.ucar.edu/models/atm-cam/docs/usersguide/node25.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 12 16:34:44 +08 2018\n",
      "year = 1850\n",
      "  Copying surface pressure file\n",
      "  Calculating mass of air in each model level\n",
      "Tue Jun 12 16:35:28 +08 2018\n",
      "  aerosol = OC\n",
      "    Copying the file for mOC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of OC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cOC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cOC_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mOC.nc\n",
      "    Removing temp_marc_1850_mass_OC.nc\n",
      "    Removing temp_marc_1850_column_OC.nc\n",
      "Tue Jun 12 16:36:17 +08 2018\n",
      "  aerosol = MOS\n",
      "    Copying the file for mMOS\n",
      "    Appending mass_air\n",
      "    Calculating the mass of MOS in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cMOS_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cMOS_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mMOS.nc\n",
      "    Removing temp_marc_1850_mass_MOS.nc\n",
      "    Removing temp_marc_1850_column_MOS.nc\n",
      "Tue Jun 12 16:37:06 +08 2018\n",
      "  aerosol = OIM\n",
      "    Copying the file for mOIM\n",
      "    Appending mass_air\n",
      "    Calculating the mass of OIM in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cOIM_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cOIM_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mOIM.nc\n",
      "    Removing temp_marc_1850_mass_OIM.nc\n",
      "    Removing temp_marc_1850_column_OIM.nc\n",
      "Tue Jun 12 16:37:53 +08 2018\n",
      "  aerosol = BC\n",
      "    Copying the file for mBC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of BC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cBC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cBC_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mBC.nc\n",
      "    Removing temp_marc_1850_mass_BC.nc\n",
      "    Removing temp_marc_1850_column_BC.nc\n",
      "Tue Jun 12 16:38:39 +08 2018\n",
      "  aerosol = MBS\n",
      "    Copying the file for mMBS\n",
      "    Appending mass_air\n",
      "    Calculating the mass of MBS in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cMBS_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cMBS_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mMBS.nc\n",
      "    Removing temp_marc_1850_mass_MBS.nc\n",
      "    Removing temp_marc_1850_column_MBS.nc\n",
      "Tue Jun 12 16:39:27 +08 2018\n",
      "  aerosol = BIM\n",
      "    Copying the file for mBIM\n",
      "    Appending mass_air\n",
      "    Calculating the mass of BIM in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cBIM_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cBIM_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mBIM.nc\n",
      "    Removing temp_marc_1850_mass_BIM.nc\n",
      "    Removing temp_marc_1850_column_BIM.nc\n",
      "Tue Jun 12 16:40:18 +08 2018\n",
      "  aerosol = NUC\n",
      "    Copying the file for mNUC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of NUC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cNUC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cNUC_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mNUC.nc\n",
      "    Removing temp_marc_1850_mass_NUC.nc\n",
      "    Removing temp_marc_1850_column_NUC.nc\n",
      "Tue Jun 12 16:41:11 +08 2018\n",
      "  aerosol = AIT\n",
      "    Copying the file for mAIT\n",
      "    Appending mass_air\n",
      "    Calculating the mass of AIT in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cAIT_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cAIT_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mAIT.nc\n",
      "    Removing temp_marc_1850_mass_AIT.nc\n",
      "    Removing temp_marc_1850_column_AIT.nc\n",
      "Tue Jun 12 16:42:12 +08 2018\n",
      "  aerosol = ACC\n",
      "    Copying the file for mACC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of ACC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cACC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cACC_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_mACC.nc\n",
      "    Removing temp_marc_1850_mass_ACC.nc\n",
      "    Removing temp_marc_1850_column_ACC.nc\n",
      "Tue Jun 12 16:43:08 +08 2018\n",
      "  aerosol = SSLT01\n",
      "    Copying the file for SSLT01\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT01 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT01_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cSSLT01_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_SSLT01.nc\n",
      "    Removing temp_marc_1850_mass_SSLT01.nc\n",
      "    Removing temp_marc_1850_column_SSLT01.nc\n",
      "Tue Jun 12 16:43:59 +08 2018\n",
      "  aerosol = SSLT02\n",
      "    Copying the file for SSLT02\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT02 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT02_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cSSLT02_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_SSLT02.nc\n",
      "    Removing temp_marc_1850_mass_SSLT02.nc\n",
      "    Removing temp_marc_1850_column_SSLT02.nc\n",
      "Tue Jun 12 16:44:52 +08 2018\n",
      "  aerosol = SSLT03\n",
      "    Copying the file for SSLT03\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT03 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT03_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cSSLT03_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_SSLT03.nc\n",
      "    Removing temp_marc_1850_mass_SSLT03.nc\n",
      "    Removing temp_marc_1850_column_SSLT03.nc\n",
      "Tue Jun 12 16:45:48 +08 2018\n",
      "  aerosol = SSLT04\n",
      "    Copying the file for SSLT04\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT04 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT04_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cSSLT04_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_SSLT04.nc\n",
      "    Removing temp_marc_1850_mass_SSLT04.nc\n",
      "    Removing temp_marc_1850_column_SSLT04.nc\n",
      "Tue Jun 12 16:46:39 +08 2018\n",
      "  aerosol = DST01\n",
      "    Copying the file for DST01\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST01 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST01_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cDST01_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_DST01.nc\n",
      "    Removing temp_marc_1850_mass_DST01.nc\n",
      "    Removing temp_marc_1850_column_DST01.nc\n",
      "Tue Jun 12 16:47:37 +08 2018\n",
      "  aerosol = DST02\n",
      "    Copying the file for DST02\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST02 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST02_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cDST02_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_DST02.nc\n",
      "    Removing temp_marc_1850_mass_DST02.nc\n",
      "    Removing temp_marc_1850_column_DST02.nc\n",
      "Tue Jun 12 16:48:35 +08 2018\n",
      "  aerosol = DST03\n",
      "    Copying the file for DST03\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST03 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST03_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cDST03_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_DST03.nc\n",
      "    Removing temp_marc_1850_mass_DST03.nc\n",
      "    Removing temp_marc_1850_column_DST03.nc\n",
      "Tue Jun 12 16:49:34 +08 2018\n",
      "  aerosol = DST04\n",
      "    Copying the file for DST04\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST04 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST04_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_1850_cDST04_LDG_ANN.nc\n",
      "    Removing temp_marc_1850_DST04.nc\n",
      "    Removing temp_marc_1850_mass_DST04.nc\n",
      "    Removing temp_marc_1850_column_DST04.nc\n",
      "  Removing temp_marc_1850_PS.nc\n",
      "  Removing temp_marc_1850_mass_air.nc\n",
      "  Removing temp_marc_1850.nco\n",
      "Tue Jun 12 16:50:32 +08 2018\n",
      "year = 2000\n",
      "  Copying surface pressure file\n",
      "  Calculating mass of air in each model level\n",
      "Tue Jun 12 16:51:14 +08 2018\n",
      "  aerosol = OC\n",
      "    Copying the file for mOC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of OC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cOC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cOC_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mOC.nc\n",
      "    Removing temp_marc_2000_mass_OC.nc\n",
      "    Removing temp_marc_2000_column_OC.nc\n",
      "Tue Jun 12 16:52:02 +08 2018\n",
      "  aerosol = MOS\n",
      "    Copying the file for mMOS\n",
      "    Appending mass_air\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Calculating the mass of MOS in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cMOS_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cMOS_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mMOS.nc\n",
      "    Removing temp_marc_2000_mass_MOS.nc\n",
      "    Removing temp_marc_2000_column_MOS.nc\n",
      "Tue Jun 12 16:52:56 +08 2018\n",
      "  aerosol = OIM\n",
      "    Copying the file for mOIM\n",
      "    Appending mass_air\n",
      "    Calculating the mass of OIM in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cOIM_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cOIM_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mOIM.nc\n",
      "    Removing temp_marc_2000_mass_OIM.nc\n",
      "    Removing temp_marc_2000_column_OIM.nc\n",
      "Tue Jun 12 16:53:46 +08 2018\n",
      "  aerosol = BC\n",
      "    Copying the file for mBC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of BC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cBC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cBC_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mBC.nc\n",
      "    Removing temp_marc_2000_mass_BC.nc\n",
      "    Removing temp_marc_2000_column_BC.nc\n",
      "Tue Jun 12 16:54:35 +08 2018\n",
      "  aerosol = MBS\n",
      "    Copying the file for mMBS\n",
      "    Appending mass_air\n",
      "    Calculating the mass of MBS in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cMBS_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cMBS_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mMBS.nc\n",
      "    Removing temp_marc_2000_mass_MBS.nc\n",
      "    Removing temp_marc_2000_column_MBS.nc\n",
      "Tue Jun 12 16:55:32 +08 2018\n",
      "  aerosol = BIM\n",
      "    Copying the file for mBIM\n",
      "    Appending mass_air\n",
      "    Calculating the mass of BIM in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cBIM_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cBIM_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mBIM.nc\n",
      "    Removing temp_marc_2000_mass_BIM.nc\n",
      "    Removing temp_marc_2000_column_BIM.nc\n",
      "Tue Jun 12 16:56:21 +08 2018\n",
      "  aerosol = NUC\n",
      "    Copying the file for mNUC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of NUC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cNUC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cNUC_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mNUC.nc\n",
      "    Removing temp_marc_2000_mass_NUC.nc\n",
      "    Removing temp_marc_2000_column_NUC.nc\n",
      "Tue Jun 12 16:57:09 +08 2018\n",
      "  aerosol = AIT\n",
      "    Copying the file for mAIT\n",
      "    Appending mass_air\n",
      "    Calculating the mass of AIT in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cAIT_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cAIT_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mAIT.nc\n",
      "    Removing temp_marc_2000_mass_AIT.nc\n",
      "    Removing temp_marc_2000_column_AIT.nc\n",
      "Tue Jun 12 16:58:05 +08 2018\n",
      "  aerosol = ACC\n",
      "    Copying the file for mACC\n",
      "    Appending mass_air\n",
      "    Calculating the mass of ACC in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cACC_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cACC_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_mACC.nc\n",
      "    Removing temp_marc_2000_mass_ACC.nc\n",
      "    Removing temp_marc_2000_column_ACC.nc\n",
      "Tue Jun 12 16:58:56 +08 2018\n",
      "  aerosol = SSLT01\n",
      "    Copying the file for SSLT01\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT01 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT01_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cSSLT01_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_SSLT01.nc\n",
      "    Removing temp_marc_2000_mass_SSLT01.nc\n",
      "    Removing temp_marc_2000_column_SSLT01.nc\n",
      "Tue Jun 12 16:59:50 +08 2018\n",
      "  aerosol = SSLT02\n",
      "    Copying the file for SSLT02\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT02 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT02_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cSSLT02_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_SSLT02.nc\n",
      "    Removing temp_marc_2000_mass_SSLT02.nc\n",
      "    Removing temp_marc_2000_column_SSLT02.nc\n",
      "Tue Jun 12 17:00:45 +08 2018\n",
      "  aerosol = SSLT03\n",
      "    Copying the file for SSLT03\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT03 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT03_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cSSLT03_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_SSLT03.nc\n",
      "    Removing temp_marc_2000_mass_SSLT03.nc\n",
      "    Removing temp_marc_2000_column_SSLT03.nc\n",
      "Tue Jun 12 17:01:37 +08 2018\n",
      "  aerosol = SSLT04\n",
      "    Copying the file for SSLT04\n",
      "    Appending mass_air\n",
      "    Calculating the mass of SSLT04 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cSSLT04_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cSSLT04_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_SSLT04.nc\n",
      "    Removing temp_marc_2000_mass_SSLT04.nc\n",
      "    Removing temp_marc_2000_column_SSLT04.nc\n",
      "Tue Jun 12 17:02:29 +08 2018\n",
      "  aerosol = DST01\n",
      "    Copying the file for DST01\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST01 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST01_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cDST01_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_DST01.nc\n",
      "    Removing temp_marc_2000_mass_DST01.nc\n",
      "    Removing temp_marc_2000_column_DST01.nc\n",
      "Tue Jun 12 17:03:23 +08 2018\n",
      "  aerosol = DST02\n",
      "    Copying the file for DST02\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST02 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST02_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cDST02_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_DST02.nc\n",
      "    Removing temp_marc_2000_mass_DST02.nc\n",
      "    Removing temp_marc_2000_column_DST02.nc\n",
      "Tue Jun 12 17:04:15 +08 2018\n",
      "  aerosol = DST03\n",
      "    Copying the file for DST03\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST03 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST03_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cDST03_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_DST03.nc\n",
      "    Removing temp_marc_2000_mass_DST03.nc\n",
      "    Removing temp_marc_2000_column_DST03.nc\n",
      "Tue Jun 12 17:05:09 +08 2018\n",
      "  aerosol = DST04\n",
      "    Copying the file for DST04\n",
      "    Appending mass_air\n",
      "    Calculating the mass of DST04 in each model level\n",
      "    Summing over levels\n",
      "    Renaming variable to cDST04_LDG\n",
      "    Setting units and long_name\n",
      "    Calculating annual means\n",
      "    Written marc_2000_cDST04_LDG_ANN.nc\n",
      "    Removing temp_marc_2000_DST04.nc\n",
      "    Removing temp_marc_2000_mass_DST04.nc\n",
      "    Removing temp_marc_2000_column_DST04.nc\n",
      "  Removing temp_marc_2000_PS.nc\n",
      "  Removing temp_marc_2000_mass_air.nc\n",
      "  Removing temp_marc_2000.nco\n",
      "Tue Jun 12 17:06:01 +08 2018\n"
     ]
    }
   ],
   "source": [
    "# Calculate column loadings from mass mixing ratios\n",
    "for year in ['1850', '2000']:  # loop over emission years\n",
    "    ! date\n",
    "    print('year = {}'.format(year))\n",
    "    # Copy the surface pressure file - necessary for decoding of the hybrid coordinates\n",
    "    print('  Copying surface pressure file')\n",
    "    in_filename = '{}/p17c_marc_{}.cam.h0.PS.nc'.format(in_dir, year)\n",
    "    ps_filename = '{}/temp_marc_{}_PS.nc'.format(out_dir, year)\n",
    "    shutil.copy2(in_filename, ps_filename)\n",
    "    # Create file containing NCO commands for calculation of air mass in each model level\n",
    "    nco_filename = '{}/temp_marc_{}.nco'.format(out_dir, year)\n",
    "    nco_file = open(nco_filename, 'w')\n",
    "    nco_file.writelines(['*P_bnds[time,ilev,lat,lon]=hyai*P0+hybi*PS;\\n',  # pressures at bounds\n",
    "                         '*P_delta[time,lev,lat,lon]=P_bnds(:,1:30,:,:)-P_bnds(:,0:29,:,:);\\n',  # deltas\n",
    "                         'mass_air=P_delta/9.807;'])  # mass of air\n",
    "    nco_file.close()\n",
    "    # Calculate mass of air in each model level\n",
    "    print('  Calculating mass of air in each model level')\n",
    "    mass_air_filename = '{}/temp_marc_{}_mass_air.nc'.format(out_dir, year)\n",
    "    ! ncap2 -O -v -S {nco_filename} {ps_filename} {mass_air_filename}\n",
    "    # Loop over mass mixing ratios for different aerosol components\n",
    "    for aerosol in ['OC', 'MOS', 'OIM', 'BC', 'MBS', 'BIM',\n",
    "                    'NUC', 'AIT', 'ACC',\n",
    "                    'SSLT01', 'SSLT02', 'SSLT03', 'SSLT04',  # sea-salt\n",
    "                    'DST01', 'DST02', 'DST03', 'DST04']:  # dust\n",
    "        ! date\n",
    "        print('  aerosol = {}'.format(aerosol))\n",
    "        # Name of corresponding mass mixing ratio variable\n",
    "        if aerosol[0:3] in ['SSL', 'DST']:\n",
    "            mmr_aerosol = aerosol\n",
    "        else:\n",
    "            mmr_aerosol = 'm{}'.format(aerosol)\n",
    "        # Copy the mass mixing ratio file\n",
    "        print('    Copying the file for {}'.format(mmr_aerosol))\n",
    "        in_filename = '{}/p17c_marc_{}.cam.h0.{}.nc'.format(in_dir, year, mmr_aerosol)\n",
    "        mmr_filename = '{}/temp_marc_{}_{}.nc'.format(out_dir, year, mmr_aerosol)\n",
    "        shutil.copy2(in_filename, mmr_filename)\n",
    "        # Append the mass of air in each model level\n",
    "        print('    Appending mass_air')\n",
    "        ! ncks -A {mass_air_filename} {mmr_filename}\n",
    "        # Calculate the mass of the aerosol\n",
    "        print('    Calculating the mass of {} in each model level'.format(aerosol))\n",
    "        mass_aerosol_filename = '{}/temp_marc_{}_mass_{}.nc'.format(out_dir, year, aerosol)\n",
    "        ! ncap2 -O -s 'mass_{aerosol}=mass_air*{mmr_aerosol}' {mmr_filename} {mass_aerosol_filename}\n",
    "        # Sum over levels to calculate column loading (and exclude unwanted variables)\n",
    "        print('    Summing over levels')\n",
    "        column_filename = '{}/temp_marc_{}_column_{}.nc'.format(out_dir, year, aerosol)\n",
    "        ! ncwa -O -x -v mass_air,{mmr_aerosol} -a lev -y sum {mass_aerosol_filename} {column_filename}\n",
    "        # Rename variable\n",
    "        print('    Renaming variable to c{}_LDG'.format(aerosol))\n",
    "        ! ncrename -v mass_{aerosol},c{aerosol}_LDG {column_filename} >/dev/null 2>/dev/null\n",
    "        # Set units and long_name\n",
    "        print('    Setting units and long_name')\n",
    "        ! ncatted -a 'units',c{aerosol}_LDG,o,c,'kg/m2' {column_filename}\n",
    "        ! ncatted -a 'long_name',c{aerosol}_LDG,o,c,'{aerosol} column loading' {column_filename}\n",
    "        # Calculate annual means (with years starting in January)\n",
    "        print('    Calculating annual means')\n",
    "        annual_filename = '{}/marc_{}_c{}_LDG_ANN.nc'.format(out_dir, year, aerosol)\n",
    "        ! ncra -O --mro -d time,,,12,12 {column_filename} {annual_filename}\n",
    "        print('    Written {}'.format(annual_filename.split('/')[-1]))\n",
    "        # Remove three temporary files\n",
    "        for filename in [mmr_filename, mass_aerosol_filename, column_filename]:\n",
    "            print('    Removing {}'.format(filename.split('/')[-1]))\n",
    "            os.remove(filename)\n",
    "    # Remove another two temporary files\n",
    "    for filename in [ps_filename, mass_air_filename, nco_filename]:\n",
    "            print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "            os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Date     Time   Level Gridsize    Miss    Diff : S Z  Max_Absdiff Max_Reldiff : Parameter name\n",
      "    16 : 0001-07-16 22:00:00       0    13824       0   13824 : F F   1.8680e-08   0.0043520 : OC_LDG     \n",
      "    28 : 0002-07-16 22:00:00       0    13824       0   13824 : F F   1.7920e-08   0.0033875 : OC_LDG     \n",
      "    40 : 0003-07-16 22:00:00       0    13824       0   13824 : F F   2.2142e-08   0.0034263 : OC_LDG     \n",
      "    52 : 0004-07-16 22:00:00       0    13824       0   13824 : F F   2.0513e-08   0.0037291 : OC_LDG     \n",
      "    64 : 0005-07-16 22:00:00       0    13824       0   13824 : F F   2.2773e-08   0.0037233 : OC_LDG     \n",
      "    76 : 0006-07-16 22:00:00       0    13824       0   13824 : F F   2.0662e-08   0.0039365 : OC_LDG     \n",
      "    88 : 0007-07-16 22:00:00       0    13824       0   13824 : F F   1.9976e-08   0.0037408 : OC_LDG     \n",
      "   100 : 0008-07-16 22:00:00       0    13824       0   13824 : F F   2.2041e-08   0.0038573 : OC_LDG     \n",
      "   112 : 0009-07-16 22:00:00       0    13824       0   13824 : F F   2.6660e-08   0.0043534 : OC_LDG     \n",
      "   124 : 0010-07-16 22:00:00       0    13824       0   13824 : F F   1.9852e-08   0.0033552 : OC_LDG     \n",
      "   136 : 0011-07-16 22:00:00       0    13824       0   13824 : F F   1.9518e-08   0.0032689 : OC_LDG     \n",
      "   148 : 0012-07-16 22:00:00       0    13824       0   13824 : F F   1.9910e-08   0.0031571 : OC_LDG     \n",
      "   160 : 0013-07-16 22:00:00       0    13824       0   13824 : F F   2.5326e-08   0.0040345 : OC_LDG     \n",
      "   172 : 0014-07-16 22:00:00       0    13824       0   13824 : F F   2.4279e-08   0.0044787 : OC_LDG     \n",
      "   184 : 0015-07-16 22:00:00       0    13824       0   13824 : F F   2.0370e-08   0.0032296 : OC_LDG     \n",
      "   196 : 0016-07-16 22:00:00       0    13824       0   13824 : F F   1.9202e-08   0.0032223 : OC_LDG     \n",
      "   208 : 0017-07-16 22:00:00       0    13824       0   13824 : F F   1.7970e-08   0.0030263 : OC_LDG     \n",
      "   220 : 0018-07-16 22:00:00       0    13824       0   13824 : F F   1.9617e-08   0.0033901 : OC_LDG     \n",
      "   232 : 0019-07-16 22:00:00       0    13824       0   13824 : F F   1.7446e-08   0.0036935 : OC_LDG     \n",
      "   244 : 0020-07-16 22:00:00       0    13824       0   13824 : F F   1.6583e-08   0.0025758 : OC_LDG     \n",
      "   256 : 0021-07-16 22:00:00       0    13824       0   13824 : F F   1.5374e-08   0.0031978 : OC_LDG     \n",
      "   268 : 0022-07-16 22:00:00       0    13824       0   13824 : F F   1.9021e-08   0.0028473 : OC_LDG     \n",
      "   280 : 0023-07-16 22:00:00       0    13824       0   13824 : F F   2.3440e-08   0.0036320 : OC_LDG     \n",
      "   292 : 0024-07-16 22:00:00       0    13824       0   13824 : F F   1.7279e-08   0.0034595 : OC_LDG     \n",
      "   304 : 0025-07-16 22:00:00       0    13824       0   13824 : F F   1.8309e-08   0.0032554 : OC_LDG     \n",
      "   316 : 0026-07-16 22:00:00       0    13824       0   13824 : F F   2.0346e-08   0.0034018 : OC_LDG     \n",
      "   328 : 0027-07-16 22:00:00       0    13824       0   13824 : F F   1.8497e-08   0.0030537 : OC_LDG     \n",
      "   340 : 0028-07-16 22:00:00       0    13824       0   13824 : F F   1.6972e-08   0.0035486 : OC_LDG     \n",
      "   352 : 0029-07-16 22:00:00       0    13824       0   13824 : F F   2.2272e-08   0.0037766 : OC_LDG     \n",
      "   364 : 0030-07-16 22:00:00       0    13824       0   13824 : F F   2.4819e-08   0.0039871 : OC_LDG     \n",
      "   376 : 0031-07-16 22:00:00       0    13824       0   13824 : F F   2.0455e-08   0.0032542 : OC_LDG     \n",
      "   388 : 0032-07-16 22:00:00       0    13824       0   13824 : F F   2.0627e-08   0.0037408 : OC_LDG     \n",
      "  32 of 388 records differ\n",
      "  0 of 388 records differ more than 0.001\n",
      "cdo diffn: Processed 886206 values from 32 variables over 64 timesteps ( 0.09s )\n",
      "  Removing temp_marc_1850_cOC_LDG_ANN.nc\n",
      "               Date     Time   Level Gridsize    Miss    Diff : S Z  Max_Absdiff Max_Reldiff : Parameter name\n",
      "    16 : 0001-07-16 22:00:00       0    13824       0   13824 : F F   1.6707e-08   0.0036111 : OC_LDG     \n",
      "    28 : 0002-07-16 22:00:00       0    13824       0   13824 : F F   1.8736e-08   0.0029704 : OC_LDG     \n",
      "    40 : 0003-07-16 22:00:00       0    13824       0   13824 : F F   2.1273e-08   0.0032537 : OC_LDG     \n",
      "    52 : 0004-07-16 22:00:00       0    13824       0   13824 : F F   1.7069e-08   0.0027961 : OC_LDG     \n",
      "    64 : 0005-07-16 22:00:00       0    13824       0   13824 : F F   1.6487e-08   0.0033589 : OC_LDG     \n",
      "    76 : 0006-07-16 22:00:00       0    13824       0   13824 : F F   1.7793e-08   0.0034047 : OC_LDG     \n",
      "    88 : 0007-07-16 22:00:00       0    13824       0   13824 : F F   1.8747e-08   0.0029713 : OC_LDG     \n",
      "   100 : 0008-07-16 22:00:00       0    13824       0   13824 : F F   1.7960e-08   0.0038876 : OC_LDG     \n",
      "   112 : 0009-07-16 22:00:00       0    13824       0   13824 : F F   1.7181e-08   0.0035497 : OC_LDG     \n",
      "   124 : 0010-07-16 22:00:00       0    13824       0   13824 : F F   1.6984e-08   0.0038519 : OC_LDG     \n",
      "   136 : 0011-07-16 22:00:00       0    13824       0   13824 : F F   1.7805e-08   0.0032660 : OC_LDG     \n",
      "   148 : 0012-07-16 22:00:00       0    13824       0   13824 : F F   2.2781e-08   0.0034432 : OC_LDG     \n",
      "   160 : 0013-07-16 22:00:00       0    13824       0   13824 : F F   1.5394e-08   0.0035602 : OC_LDG     \n",
      "   172 : 0014-07-16 22:00:00       0    13824       0   13824 : F F   1.4387e-08   0.0026496 : OC_LDG     \n",
      "   184 : 0015-07-16 22:00:00       0    13824       0   13824 : F F   1.5646e-08   0.0030676 : OC_LDG     \n",
      "   196 : 0016-07-16 22:00:00       0    13824       0   13824 : F F   1.8803e-08   0.0032195 : OC_LDG     \n",
      "   208 : 0017-07-16 22:00:00       0    13824       0   13824 : F F   1.5704e-08   0.0029909 : OC_LDG     \n",
      "   220 : 0018-07-16 22:00:00       0    13824       0   13824 : F F   1.5839e-08   0.0037267 : OC_LDG     \n",
      "   232 : 0019-07-16 22:00:00       0    13824       0   13824 : F F   2.0219e-08   0.0036644 : OC_LDG     \n",
      "   244 : 0020-07-16 22:00:00       0    13824       0   13824 : F F   1.4081e-08   0.0029280 : OC_LDG     \n",
      "   256 : 0021-07-16 22:00:00       0    13824       0   13824 : F F   1.9743e-08   0.0028092 : OC_LDG     \n",
      "   268 : 0022-07-16 22:00:00       0    13824       0   13824 : F F   1.5901e-08   0.0034801 : OC_LDG     \n",
      "   280 : 0023-07-16 22:00:00       0    13824       0   13824 : F F   1.7091e-08   0.0031632 : OC_LDG     \n",
      "   292 : 0024-07-16 22:00:00       0    13824       0   13824 : F F   1.7592e-08   0.0034473 : OC_LDG     \n",
      "   304 : 0025-07-16 22:00:00       0    13824       0   13824 : F F   1.7532e-08   0.0033852 : OC_LDG     \n",
      "   316 : 0026-07-16 22:00:00       0    13824       0   13824 : F F   1.4760e-08   0.0037670 : OC_LDG     \n",
      "   328 : 0027-07-16 22:00:00       0    13824       0   13824 : F F   2.0217e-08   0.0031503 : OC_LDG     \n",
      "   340 : 0028-07-16 22:00:00       0    13824       0   13824 : F F   1.7305e-08   0.0032052 : OC_LDG     \n",
      "   352 : 0029-07-16 22:00:00       0    13824       0   13824 : F F   1.9433e-08   0.0027304 : OC_LDG     \n",
      "   364 : 0030-07-16 22:00:00       0    13824       0   13824 : F F   1.7809e-08   0.0032860 : OC_LDG     \n",
      "   376 : 0031-07-16 22:00:00       0    13824       0   13824 : F F   1.7248e-08   0.0035413 : OC_LDG     \n",
      "   388 : 0032-07-16 22:00:00       0    13824       0   13824 : F F   1.3613e-08   0.0029042 : OC_LDG     \n",
      "  32 of 388 records differ\n",
      "  0 of 388 records differ more than 0.001\n",
      "cdo diffn: Processed 886206 values from 32 variables over 64 timesteps ( 0.09s )\n",
      "  Removing temp_marc_2000_cOC_LDG_ANN.nc\n",
      "Tue Jun 12 17:06:02 +08 2018\n"
     ]
    }
   ],
   "source": [
    "# Compare cOC_LDG (calculated above) with standard OC_LDG\n",
    "for year in ['1850', '2000']:\n",
    "    # Input files\n",
    "    in_filename_1 = '{}/marc_{}_OC_LDG_ANN.nc'.format(out_dir, year)\n",
    "    in_filename_2 = '{}/marc_{}_cOC_LDG_ANN.nc'.format(out_dir, year)\n",
    "    # Rename cOC_LDG to OC_LDG to enable comparison\n",
    "    temp_filename = '{}/temp_marc_{}_cOC_LDG_ANN.nc'.format(out_dir, year)\n",
    "    ! ncrename -O -v cOC_LDG,OC_LDG {in_filename_2} {temp_filename} >/dev/null 2>/dev/null\n",
    "    # Compare using CDO\n",
    "    ! cdo diffv {in_filename_1} {temp_filename}\n",
    "    # Remove temporary file\n",
    "    for filename in [temp_filename, ]:\n",
    "            print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "            os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In results above, the maximum relative difference is less than 1%. This shows that the calculation of the column loadings is working as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive additional atmosphere variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctOC_LDG=cOC_LDG+cOIM_LDG, marc, 1850\n",
      "  Written marc_1850_ctOC_LDG_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "ctOC_LDG=cOC_LDG+cOIM_LDG, marc, 2000\n",
      "  Written marc_2000_ctOC_LDG_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "ctBC_LDG=cBC_LDG+cBIM_LDG, marc, 1850\n",
      "  Written marc_1850_ctBC_LDG_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "ctBC_LDG=cBC_LDG+cBIM_LDG, marc, 2000\n",
      "  Written marc_2000_ctBC_LDG_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cPRECT=PRECC+PRECL, marc, 1850\n",
      "  Written marc_1850_cPRECT_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "cPRECT=PRECC+PRECL, marc, 2000\n",
      "  Written marc_2000_cPRECT_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cPRECT=PRECC+PRECL, mam3, 1850\n",
      "  Written mam3_1850_cPRECT_ANN.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "cPRECT=PRECC+PRECL, mam3, 2000\n",
      "  Written mam3_2000_cPRECT_ANN.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "cPRECT=PRECC+PRECL, mam7, 1850\n",
      "  Written mam7_1850_cPRECT_ANN.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "cPRECT=PRECC+PRECL, mam7, 2000\n",
      "  Written mam7_2000_cPRECT_ANN.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "cPRECST=PRECSC+PRECSL, marc, 1850\n",
      "  Written marc_1850_cPRECST_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "cPRECST=PRECSC+PRECSL, marc, 2000\n",
      "  Written marc_2000_cPRECST_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cPRECST=PRECSC+PRECSL, mam3, 1850\n",
      "  Written mam3_1850_cPRECST_ANN.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "cPRECST=PRECSC+PRECSL, mam3, 2000\n",
      "  Written mam3_2000_cPRECST_ANN.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "cPRECST=PRECSC+PRECSL, mam7, 1850\n",
      "  Written mam7_1850_cPRECST_ANN.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "cPRECST=PRECSC+PRECSL, mam7, 2000\n",
      "  Written mam7_2000_cPRECST_ANN.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "cFNTOA=FSNTOA+LWCF, marc, 1850\n",
      "  Written marc_1850_cFNTOA_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "cFNTOA=FSNTOA+LWCF, marc, 2000\n",
      "  Written marc_2000_cFNTOA_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cFNTOA=FSNTOA+LWCF, mam3, 1850\n",
      "  Written mam3_1850_cFNTOA_ANN.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "cFNTOA=FSNTOA+LWCF, mam3, 2000\n",
      "  Written mam3_2000_cFNTOA_ANN.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "cFNTOA=FSNTOA+LWCF, mam7, 1850\n",
      "  Written mam7_1850_cFNTOA_ANN.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "cFNTOA=FSNTOA+LWCF, mam7, 2000\n",
      "  Written mam7_2000_cFNTOA_ANN.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "cDRE=FSNTOA-FSNTOANOA, marc, 1850\n",
      "  Written marc_1850_cDRE_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "cDRE=FSNTOA-FSNTOANOA, marc, 2000\n",
      "  Written marc_2000_cDRE_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cDRE=FSNTOA-FSNTOA_d1, mam3, 1850\n",
      "  Written mam3_1850_cDRE_ANN.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "cDRE=FSNTOA-FSNTOA_d1, mam3, 2000\n",
      "  Written mam3_2000_cDRE_ANN.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "cDRE=FSNTOA-FSNTOA_d1, mam7, 1850\n",
      "  Written mam7_1850_cDRE_ANN.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "cDRE=FSNTOA-FSNTOA_d1, mam7, 2000\n",
      "  Written mam7_2000_cDRE_ANN.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "cDREsurf=FSNS-FSNSNOA, marc, 1850\n",
      "  Written marc_1850_cDREsurf_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "cDREsurf=FSNS-FSNSNOA, marc, 2000\n",
      "  Written marc_2000_cDREsurf_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cDREsurf=FSNS-FSNS_d1, mam3, 1850\n",
      "  Written mam3_1850_cDREsurf_ANN.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "cDREsurf=FSNS-FSNS_d1, mam3, 2000\n",
      "  Written mam3_2000_cDREsurf_ANN.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "cDREsurf=FSNS-FSNS_d1, mam7, 1850\n",
      "  Written mam7_1850_cDREsurf_ANN.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "cDREsurf=FSNS-FSNS_d1, mam7, 2000\n",
      "  Written mam7_2000_cDREsurf_ANN.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "cAAA=cDRE-cDREsurf, marc, 1850\n",
      "  Written marc_1850_cAAA_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "cAAA=cDRE-cDREsurf, marc, 2000\n",
      "  Written marc_2000_cAAA_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "cAAA=cDRE-cDREsurf, mam3, 1850\n",
      "  Written mam3_1850_cAAA_ANN.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "cAAA=cDRE-cDREsurf, mam3, 2000\n",
      "  Written mam3_2000_cAAA_ANN.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "cAAA=cDRE-cDREsurf, mam7, 1850\n",
      "  Written mam7_1850_cAAA_ANN.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "cAAA=cDRE-cDREsurf, mam7, 2000\n",
      "  Written mam7_2000_cAAA_ANN.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "Tue Jun 12 17:06:18 +08 2018\n"
     ]
    }
   ],
   "source": [
    "# Derived variables that require *two* input variables\n",
    "derived_variable_dict = {'ctOC_LDG=cOC_LDG+cOIM_LDG': ['marc',],  # total OC loading\n",
    "                         'ctBC_LDG=cBC_LDG+cBIM_LDG': ['marc',],  # total BC loading\n",
    "                         'cPRECT=PRECC+PRECL': ['marc', 'mam3', 'mam7'],  # total precipitation rate\n",
    "                         'cPRECST=PRECSC+PRECSL': ['marc', 'mam3', 'mam7'],  # total snow rate\n",
    "                         'cFNTOA=FSNTOA+LWCF': ['marc', 'mam3', 'mam7'],  # net (SW + LW) radiative effect\n",
    "                         'cDRE=FSNTOA-FSNTOANOA': ['marc',],  # direct radiative effect at TOA\n",
    "                         'cDRE=FSNTOA-FSNTOA_d1': ['mam3', 'mam7'],\n",
    "                         'cDREsurf=FSNS-FSNSNOA': ['marc',],  # direct radiative effect at surface\n",
    "                         'cDREsurf=FSNS-FSNS_d1': ['mam3', 'mam7'],\n",
    "                         'cAAA=cDRE-cDREsurf': ['marc', 'mam3', 'mam7'],  # absorption by aerosols in atmosphere\n",
    "                        }\n",
    "for derived_variable, model_list in derived_variable_dict.items():\n",
    "    for model in model_list:\n",
    "        year_list = ['1850', '2000']\n",
    "        for year in year_list:\n",
    "            print('{}, {}, {}'.format(derived_variable, model, year))\n",
    "            # Merge input files\n",
    "            variable_list = re.split('\\=|\\+|\\-', derived_variable)\n",
    "            in_filename_1 = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable_list[1])\n",
    "            in_filename_2 = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable_list[2])\n",
    "            merge_filename = '{}/temp_{}_{}_merge_ANN.nc'.format(out_dir, model, year)\n",
    "            ! cdo -s merge {in_filename_1} {in_filename_2} {merge_filename} >/dev/null 2>/dev/null\n",
    "            # Calculate derived variable\n",
    "            out_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable_list[0])\n",
    "            ! cdo -s expr,'{derived_variable}' {merge_filename} {out_filename}\n",
    "            if os.path.isfile(out_filename):\n",
    "                print('  Written {}'.format(out_filename.split('/')[-1]))\n",
    "            # Remove temporary file\n",
    "            for filename in [merge_filename, ]:\n",
    "                print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "                os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctSSLT_LDG=cSSLT01_LDG+cSSLT02_LDG+cSSLT03_LDG+cSSLT04_LDG, marc, 1850\n",
      "  Written marc_1850_ctSSLT_LDG_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "ctSSLT_LDG=cSSLT01_LDG+cSSLT02_LDG+cSSLT03_LDG+cSSLT04_LDG, marc, 2000\n",
      "  Written marc_2000_ctSSLT_LDG_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "ctDST_LDG=cDST01_LDG+cDST02_LDG+cDST03_LDG+cDST04_LDG, marc, 1850\n",
      "  Written marc_1850_ctDST_LDG_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "ctDST_LDG=cDST01_LDG+cDST02_LDG+cDST03_LDG+cDST04_LDG, marc, 2000\n",
      "  Written marc_2000_ctDST_LDG_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "Tue Jun 12 17:06:21 +08 2018\n"
     ]
    }
   ],
   "source": [
    "# ctSSLT_LDG and ctDST_LDG require *four* input variables\n",
    "for sslt_or_dst in ['SSLT', 'DST']:  # sea-salt or dust\n",
    "    variable_list = ['ct{}_LDG'.format(sslt_or_dst), ]\n",
    "    for size_bin in ['01', '02', '03', '04']:\n",
    "        variable_list.append('c{}{}_LDG'.format(sslt_or_dst, size_bin))\n",
    "    derived_variable = '{}={}+{}+{}+{}'.format(*variable_list)\n",
    "    model = 'marc'  # MARC only\n",
    "    year_list = ['1850', '2000']\n",
    "    for year in year_list:\n",
    "        print('{}, {}, {}'.format(derived_variable, model, year))\n",
    "        # Merge input files\n",
    "        in_filename_list = []\n",
    "        for variable in variable_list[1:]:\n",
    "            in_filename_list.append('{}/{}_{}_{}_ANN.nc'.format(out_dir, model,\n",
    "                                                                year, variable))\n",
    "        merge_filename = '{}/temp_{}_{}_merge_ANN.nc'.format(out_dir, model, year)\n",
    "        ! cdo -s merge {in_filename_list[0]} {in_filename_list[1]} \\\n",
    "            {in_filename_list[2]} {in_filename_list[3]} {merge_filename} >/dev/null 2>/dev/null\n",
    "        # Calculate derived variable\n",
    "        out_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable_list[0])\n",
    "        ! cdo -s expr,'{derived_variable}' {merge_filename} {out_filename}\n",
    "        if os.path.isfile(out_filename):\n",
    "            print('  Written {}'.format(out_filename.split('/')[-1]))\n",
    "        # Remove temporary file\n",
    "        for filename in [merge_filename, ]:\n",
    "            print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "            os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctSUL_LDG=cACC_LDG+cAIT_LDG+cNUC_LDG+cMOS_LDG+cMBS_LDG-cOIM_LDG-cBIM_LDG, marc, 1850\n",
      "  Written marc_1850_ctSUL_LDG_ANN.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "ctSUL_LDG=cACC_LDG+cAIT_LDG+cNUC_LDG+cMOS_LDG+cMBS_LDG-cOIM_LDG-cBIM_LDG, marc, 2000\n",
      "  Written marc_2000_ctSUL_LDG_ANN.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "Tue Jun 12 17:08:47 +08 2018\n"
     ]
    }
   ],
   "source": [
    "# ctSUL_LDG requires *seven* input variables\n",
    "derived_variable_dict = {'ctSUL_LDG=cACC_LDG+cAIT_LDG+cNUC_LDG+cMOS_LDG+cMBS_LDG-cOIM_LDG-cBIM_LDG':\n",
    "                         ['marc',],}  # total SO4 loading\n",
    "for derived_variable, model_list in derived_variable_dict.items():\n",
    "    for model in model_list:\n",
    "        year_list = ['1850', '2000']\n",
    "        for year in year_list:\n",
    "            print('{}, {}, {}'.format(derived_variable, model, year))\n",
    "            # Merge input files\n",
    "            variable_list = re.split('\\=|\\+|\\-', derived_variable)\n",
    "            in_filename_list = []\n",
    "            for variable in variable_list[1:]:\n",
    "                in_filename_list.append('{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable))\n",
    "            merge_filename = '{}/temp_{}_{}_merge_ANN.nc'.format(out_dir, model, year)\n",
    "            ! cdo -s merge {in_filename_list[0]} {in_filename_list[1]} {in_filename_list[2]} \\\n",
    "               {in_filename_list[3]} {in_filename_list[4]} {in_filename_list[5]} \\\n",
    "               {in_filename_list[6]} {merge_filename} >/dev/null 2>/dev/null\n",
    "            # Calculate derived variable\n",
    "            out_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable_list[0])\n",
    "            ! cdo -s expr,'{derived_variable}' {merge_filename} {out_filename}\n",
    "            if os.path.isfile(out_filename):\n",
    "                print('  Written {}'.format(out_filename.split('/')[-1]))\n",
    "            # Remove temporary file\n",
    "            for filename in [merge_filename, ]:\n",
    "                print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "                os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate annual means of land variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSNO, marc, 1850\n",
      "  Written marc_1850_FSNO_ANN.nc\n",
      "  Removing temp_marc_1850_FSNO_ANN.nc\n",
      "FSNO, marc, 2000\n",
      "  Written marc_2000_FSNO_ANN.nc\n",
      "  Removing temp_marc_2000_FSNO_ANN.nc\n",
      "FSNO, mam3, 1850\n",
      "  Written mam3_1850_FSNO_ANN.nc\n",
      "  Removing temp_mam3_1850_FSNO_ANN.nc\n",
      "FSNO, mam3, 2000\n",
      "  Written mam3_2000_FSNO_ANN.nc\n",
      "  Removing temp_mam3_2000_FSNO_ANN.nc\n",
      "FSNO, mam7, 1850\n",
      "  Written mam7_1850_FSNO_ANN.nc\n",
      "  Removing temp_mam7_1850_FSNO_ANN.nc\n",
      "FSNO, mam7, 2000\n",
      "  Written mam7_2000_FSNO_ANN.nc\n",
      "  Removing temp_mam7_2000_FSNO_ANN.nc\n",
      "SNOBCMSL, marc, 1850\n",
      "  Written marc_1850_SNOBCMSL_ANN.nc\n",
      "  Removing temp_marc_1850_SNOBCMSL_ANN.nc\n",
      "SNOBCMSL, marc, 2000\n",
      "  Written marc_2000_SNOBCMSL_ANN.nc\n",
      "  Removing temp_marc_2000_SNOBCMSL_ANN.nc\n",
      "SNOBCMSL, mam3, 1850\n",
      "  Written mam3_1850_SNOBCMSL_ANN.nc\n",
      "  Removing temp_mam3_1850_SNOBCMSL_ANN.nc\n",
      "SNOBCMSL, mam3, 2000\n",
      "  Written mam3_2000_SNOBCMSL_ANN.nc\n",
      "  Removing temp_mam3_2000_SNOBCMSL_ANN.nc\n",
      "SNOBCMSL, mam7, 1850\n",
      "  Written mam7_1850_SNOBCMSL_ANN.nc\n",
      "  Removing temp_mam7_1850_SNOBCMSL_ANN.nc\n",
      "SNOBCMSL, mam7, 2000\n",
      "  Written mam7_2000_SNOBCMSL_ANN.nc\n",
      "  Removing temp_mam7_2000_SNOBCMSL_ANN.nc\n",
      "BCDEP, marc, 1850\n",
      "  Written marc_1850_BCDEP_ANN.nc\n",
      "  Removing temp_marc_1850_BCDEP_ANN.nc\n",
      "BCDEP, marc, 2000\n",
      "  Written marc_2000_BCDEP_ANN.nc\n",
      "  Removing temp_marc_2000_BCDEP_ANN.nc\n",
      "BCDEP, mam3, 1850\n",
      "  Written mam3_1850_BCDEP_ANN.nc\n",
      "  Removing temp_mam3_1850_BCDEP_ANN.nc\n",
      "BCDEP, mam3, 2000\n",
      "  Written mam3_2000_BCDEP_ANN.nc\n",
      "  Removing temp_mam3_2000_BCDEP_ANN.nc\n",
      "BCDEP, mam7, 1850\n",
      "  Written mam7_1850_BCDEP_ANN.nc\n",
      "  Removing temp_mam7_1850_BCDEP_ANN.nc\n",
      "BCDEP, mam7, 2000\n",
      "  Written mam7_2000_BCDEP_ANN.nc\n",
      "  Removing temp_mam7_2000_BCDEP_ANN.nc\n",
      "Tue Jun 12 17:09:06 +08 2018\n"
     ]
    }
   ],
   "source": [
    "variable_list = ['FSNO',  # fraction of ground covered by snow\n",
    "                 'SNOBCMSL',  # mass of BC in top layer of snow\n",
    "                 'BCDEP'  # total BC deposition (dry+wet) from atmosphere\n",
    "                ]\n",
    "for variable in variable_list:\n",
    "    for model in ['marc', 'mam3', 'mam7']:\n",
    "        for year in ['1850', '2000']:\n",
    "            # Check if input file exists\n",
    "            in_filename = '{}/p17c_{}_{}.clm2.h0.{}.nc'.format(in_dir, model, year, variable)\n",
    "            if os.path.isfile(in_filename):\n",
    "                print('{}, {}, {}'.format(variable, model, year))\n",
    "                # Calculate annual means using NCO (with years starting in January)\n",
    "                temp_filename = '{}/temp_{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable)\n",
    "                ! ncra -O --mro -d time,,,12,12 {in_filename} {temp_filename}\n",
    "                # Replace missing values with zero\n",
    "                out_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable)\n",
    "                ! cdo -s setmisstoc,0 {temp_filename} {out_filename}\n",
    "                print('  Written {}'.format(out_filename.split('/')[-1]))\n",
    "                # Remove temporary file\n",
    "                for filename in [temp_filename, ]:\n",
    "                    print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "                    os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate annual means of sea ice variables and remap to lonlat grid\n",
    "Note: the data are initially on a curvilinear grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written grid.txt\n",
      "Tue Jun 12 17:09:17 +08 2018\r\n"
     ]
    }
   ],
   "source": [
    "# Create grid file\n",
    "# Note: this grid is only approximately the same as the land grid\n",
    "grid_filename = '{}/grid.txt'.format(in_dir)\n",
    "grid_file = open(grid_filename, 'w')\n",
    "grid_file.writelines(['gridtype = lonlat\\n',\n",
    "                      'xsize    = 144\\n',\n",
    "                      'ysize    = 96\\n',\n",
    "                      'xfirst   = 0\\n',\n",
    "                      'xinc     = 2.5\\n',\n",
    "                      'yfirst   = -90\\n',\n",
    "                      'yinc     = 1.89473724\\n'])\n",
    "grid_file.close()\n",
    "print('Written {}'.format(grid_filename.split('/')[-1]))\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs, marc, 1850\n",
      "  Written marc_1850_fs_ANN.nc\n",
      "  Removing temp_marc_1850_fs_ANN.nc\n",
      "  Removing temp_marc_1850_fs_ANN_regrid.nc\n",
      "fs, marc, 2000\n",
      "  Written marc_2000_fs_ANN.nc\n",
      "  Removing temp_marc_2000_fs_ANN.nc\n",
      "  Removing temp_marc_2000_fs_ANN_regrid.nc\n",
      "fs, mam3, 1850\n",
      "  Written mam3_1850_fs_ANN.nc\n",
      "  Removing temp_mam3_1850_fs_ANN.nc\n",
      "  Removing temp_mam3_1850_fs_ANN_regrid.nc\n",
      "fs, mam3, 2000\n",
      "  Written mam3_2000_fs_ANN.nc\n",
      "  Removing temp_mam3_2000_fs_ANN.nc\n",
      "  Removing temp_mam3_2000_fs_ANN_regrid.nc\n",
      "fs, mam7, 1850\n",
      "  Written mam7_1850_fs_ANN.nc\n",
      "  Removing temp_mam7_1850_fs_ANN.nc\n",
      "  Removing temp_mam7_1850_fs_ANN_regrid.nc\n",
      "fs, mam7, 2000\n",
      "  Written mam7_2000_fs_ANN.nc\n",
      "  Removing temp_mam7_2000_fs_ANN.nc\n",
      "  Removing temp_mam7_2000_fs_ANN_regrid.nc\n",
      "Tue Jun 12 17:11:54 +08 2018\n"
     ]
    }
   ],
   "source": [
    "variable_list = ['fs',  # grid-cell mean snow fraction over ice\n",
    "                ]\n",
    "for variable in variable_list:\n",
    "    for model in ['marc', 'mam3', 'mam7']:\n",
    "        for year in ['1850', '2000']:\n",
    "            # Check if input file exists\n",
    "            in_filename = '{}/p17c_{}_{}.cice.h.{}.nc'.format(in_dir, model, year, variable)\n",
    "            if os.path.isfile(in_filename):\n",
    "                print('{}, {}, {}'.format(variable, model, year))\n",
    "                # Calculate annual means using NCO (with years starting in January)\n",
    "                annual_filename = '{}/temp_{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable)\n",
    "                ! ncra -O --mro -d time,,,12,12 {in_filename} {annual_filename}\n",
    "                # Apply distance weighted remapping to new grid using CDO\n",
    "                regrid_filename = '{}/temp_{}_{}_{}_ANN_regrid.nc'.format(out_dir, model, year, variable)\n",
    "                ! cdo -s remapdis,{grid_filename} {annual_filename} {regrid_filename}\n",
    "                # Replace missing values with zero\n",
    "                out_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, variable)\n",
    "                ! cdo -s setmisstoc,0 {regrid_filename} {out_filename}\n",
    "                print('  Written {}'.format(out_filename.split('/')[-1]))\n",
    "                # Remove temporary file\n",
    "                for filename in [annual_filename, regrid_filename]:\n",
    "                    print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "                    os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine snow cover over land and sea ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marc, 1850\n",
      "  Written marc_1850_cSnowCover_ANN.nc\n",
      "  Removing temp_marc_1850_FSNO_ANN_regrid.nc\n",
      "  Removing temp_marc_1850_merge_ANN.nc\n",
      "marc, 2000\n",
      "  Written marc_2000_cSnowCover_ANN.nc\n",
      "  Removing temp_marc_2000_FSNO_ANN_regrid.nc\n",
      "  Removing temp_marc_2000_merge_ANN.nc\n",
      "mam3, 1850\n",
      "  Written mam3_1850_cSnowCover_ANN.nc\n",
      "  Removing temp_mam3_1850_FSNO_ANN_regrid.nc\n",
      "  Removing temp_mam3_1850_merge_ANN.nc\n",
      "mam3, 2000\n",
      "  Written mam3_2000_cSnowCover_ANN.nc\n",
      "  Removing temp_mam3_2000_FSNO_ANN_regrid.nc\n",
      "  Removing temp_mam3_2000_merge_ANN.nc\n",
      "mam7, 1850\n",
      "  Written mam7_1850_cSnowCover_ANN.nc\n",
      "  Removing temp_mam7_1850_FSNO_ANN_regrid.nc\n",
      "  Removing temp_mam7_1850_merge_ANN.nc\n",
      "mam7, 2000\n",
      "  Written mam7_2000_cSnowCover_ANN.nc\n",
      "  Removing temp_mam7_2000_FSNO_ANN_regrid.nc\n",
      "  Removing temp_mam7_2000_merge_ANN.nc\n",
      "Tue Jun 12 17:11:58 +08 2018\n"
     ]
    }
   ],
   "source": [
    "for model in ['marc', 'mam3', 'mam7']:\n",
    "    for year in ['1850', '2000']:\n",
    "        # Check if input files exists\n",
    "        lnd_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, 'FSNO')\n",
    "        ice_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, 'fs')\n",
    "        if os.path.isfile(lnd_filename) and os.path.isfile(lnd_filename):\n",
    "            print('{}, {}'.format(model, year))\n",
    "            # Remap land data to identical grid as was used for ice\n",
    "            lnd_regrid_filename = '{}/temp_{}_{}_{}_ANN_regrid.nc'.format(out_dir, model, year, 'FSNO')\n",
    "            ! cdo -s remapnn,{grid_filename} {lnd_filename} {lnd_regrid_filename}\n",
    "            # Merge land and ice data into one file\n",
    "            merge_filename = '{}/temp_{}_{}_merge_ANN.nc'.format(out_dir, model, year)\n",
    "            ! cdo -s merge {lnd_regrid_filename} {ice_filename} {merge_filename}\n",
    "            # Combine snow cover over land and ice, weighting by land fraction\n",
    "            out_filename = '{}/{}_{}_{}_ANN.nc'.format(out_dir, model, year, 'cSnowCover')\n",
    "            derivation_str = '_oceanfrac=1-landfrac;cSnowCover=landfrac*FSNO+_oceanfrac*fs'\n",
    "            ! cdo -s expr,'{derivation_str}' {merge_filename} {out_filename}\n",
    "            print('  Written {}'.format(out_filename.split('/')[-1]))\n",
    "            # Remove temporary files\n",
    "            for filename in [lnd_regrid_filename, merge_filename]:\n",
    "                print('  Removing {}'.format(filename.split('/')[-1]))\n",
    "                os.remove(filename)\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 12 17:11:58 +08 2018\r\n"
     ]
    }
   ],
   "source": [
    "! date"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
